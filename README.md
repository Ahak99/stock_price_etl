# Projet : Pipeline ETL pour l'analyse de l'indice S&P500 ğŸš€ğŸ“Š

## Description du projet ğŸŒŸ

Ce projet vise Ã  dÃ©velopper et Ã  orchestrer une pipeline ETL (Extraction, Transformation, Chargement) pour la collecte et le traitement des donnÃ©es liÃ©es Ã  l'indice S&P500 et aux actions qui le composent. 
L'objectif est de fournir une solution automatisÃ©e pour analyser et transformer des donnÃ©es financiÃ¨res en formats exploitables pour les Ã©quipes dÃ©cisionnelles. ğŸ’¡

## FonctionnalitÃ©s principales âœ¨

- **Analyse des besoins fonctionnels** : Identification des flux de donnÃ©es et des exigences techniques.
- **Extraction des donnÃ©es** : Utilisation de l'API Yahoo Finance pour collecter des informations sur l'indice S&P500 et les actions associÃ©es.
- **Transformation et analyse des donnÃ©es** : Utilisation de Python, Pandas, et Numpy pour nettoyer, transformer, et analyser les donnÃ©es.
- **Orchestration de pipeline** : Mise en place d'une pipeline ETL automatisÃ©e Ã  l'aide d'Apache Airflow.
- **DÃ©ploiement cloud** : CrÃ©ation et gestion d'un environnement cloud complet sur AWS grÃ¢ce Ã  Terraform :
  - EC2 pour l'hÃ©bergement d'Apache Airflow et des scripts Python.
  - S3 pour le stockage des donnÃ©es brutes et transformÃ©es.

## Technologies utilisÃ©es ğŸ› ï¸

- **Langages** : Python ğŸ
- **BibliothÃ¨ques** : Pandas, Numpy ğŸ“š
- **Orchestration ETL** : Apache Airflow ğŸ”€
- **Infrastructure Cloud** : AWS (EC2, S3) â˜ï¸
- **Infrastructure as Code** : Terraform ğŸ—½ï¸
- **API** : Yahoo Finance API ğŸŒ
- **Gestion de version** : Git ğŸ› ï¸

## PrÃ©requis âœ…

- **Python 3.x**
- **Terraform**
- **AWS CLI** configurÃ© avec des accÃ¨s appropriÃ©s
- **Apache Airflow**

## Installation âš™ï¸

Toutes les Ã©tapes suivantes doivent Ãªtre effectuÃ©es **dans l'instance EC2 crÃ©Ã©e**.

1. **Mettre Ã  jour le systÃ¨me et installer Python et Pip** :
   ```bash
   sudo apt-get update
   sudo apt install -y python3-pip
   sudo apt install -y python3.12-venv
   ```

2. **CrÃ©er un environnement virtuel et l'activer** :
   ```bash
   python3 -m venv venv
   source venv/bin/activate
   ```

3. **Cloner le dÃ©pÃ´t du projet** :
   ```bash
   git clone https://github.com/Ahak99/stock_price_etl.git
   ```

4. **RÃ©organiser les fichiers du projet** :
   ```bash
   mv stock_price_etl/src/requirements.txt ./
   mv stock_price_etl/src ./
   mv stock_price_etl/infrastructure ./
   mv stock_price_etl/airflow_folder/dags airflow
   ```

5. **Installer les dÃ©pendances Python** :
   ```bash
   pip install -r requirements.txt
   ```

6. **Configurer l'environnement AWS avec Terraform** :
   - AccÃ©dez au rÃ©pertoire `infrastructure` :
     ```bash
     cd infrastructure
     ```
   - Initialisez Terraform :
     ```bash
     terraform init
     ```
   - Appliquez la configuration pour dÃ©ployer l'infrastructure :
     ```bash
     terraform apply
     ```

7. **VÃ©rifier l'installation d'Airflow** :
   ```bash
   airflow standalone
   ```

## Utilisation ğŸš€

1. **Lancer la pipeline ETL** :
   - AccÃ©dez Ã  l'interface web d'Airflow.
   - Activez et exÃ©cutez le DAG correspondant.

2. **VÃ©rification des donnÃ©es** :
   - Les donnÃ©es brutes et transformÃ©es seront disponibles dans le bucket S3 configurÃ©.
