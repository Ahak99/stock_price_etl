# Projet : Pipeline ETL pour l'analyse de l'indice S&P500 ğŸš€ğŸ“Š

## Description du projet ğŸŒŸ

Ce projet vise Ã  dÃ©velopper et Ã  orchestrer une pipeline ETL (Extraction, Transformation, Chargement) pour la collecte et le traitement des donnÃ©es liÃ©es Ã  l'indice S&P500 et aux actions qui le composent. 
L'objectif est de fournir une solution automatisÃ©e pour analyser et transformer des donnÃ©es financiÃ¨res en formats exploitables pour les Ã©quipes dÃ©cisionnelles. ğŸ’¡

## FonctionnalitÃ©s principales âœ¨

- **Analyse des besoins fonctionnels** : Identification des flux de donnÃ©es et des exigences techniques.
- **Extraction des donnÃ©es** : Utilisation de l'API Yahoo Finance pour collecter des informations sur l'indice S&P500 et les actions associÃ©es.
- **Transformation et analyse des donnÃ©es** : Utilisation de Python, Pandas, et Numpy pour nettoyer, transformer, et analyser les donnÃ©es.
- **Orchestration de pipeline** : Mise en place d'une pipeline ETL automatisÃ©e Ã  l'aide d'Apache Airflow.
- **DÃ©ploiement cloud** : CrÃ©ation et gestion d'un environnement cloud complet sur AWS grÃ¢ce Ã  Terraform :
  - EC2 pour l'hÃ©bergement d'Apache Airflow et des scripts Python.
  - S3 pour le stockage des donnÃ©es brutes et transformÃ©es.

## Technologies utilisÃ©es ğŸ› ï¸

- **Langages** : Python ğŸ
- **BibliothÃ¨ques** : Pandas, Numpy ğŸ“š
- **Orchestration ETL** : Apache Airflow ğŸŒ€
- **Infrastructure Cloud** : AWS (EC2, S3) â˜ï¸
- **Infrastructure as Code** : Terraform ğŸ—ï¸
- **API** : Yahoo Finance API ğŸŒ
- **Gestion de version** : Git ğŸ§°

## PrÃ©requis âœ…

- **Python 3.x**
- **Terraform**
- **AWS CLI** configurÃ© avec des accÃ¨s appropriÃ©s
- **Apache Airflow**

## Installation âš™ï¸

1. **Cloner le dÃ©pÃ´t** :
   ```bash
   git clone https://github.com/Ahak99/stock_price_etl.git
   cd stock_price_etl
   ```

2. **Installer les dÃ©pendances Python** :
   ```bash
   pip install -r src/stock_price_etl/requirements.txt
   ```

3. **Configurer l'environnement AWS avec Terraform** :
   - Naviguez dans le rÃ©pertoire `terraform`.
   - Initialisez Terraform :
     ```bash
     terraform init
     ```
   - Appliquez la configuration pour dÃ©ployer l'infrastructure :
     ```bash
     terraform apply
     ```

4. **Configurer Apache Airflow** :
   - Installer Airflow si ce n'est pas dÃ©jÃ  fait.
   - Ajouter les DAGs au rÃ©pertoire Airflow.
   - Lancer le service Airflow :
     ```bash
     airflow standalone
     ```

## Utilisation ğŸš€

1. **Lancer la pipeline ETL** :
   - AccÃ©dez Ã  l'interface web d'Airflow.
   - Activez et exÃ©cutez le DAG correspondant.

2. **VÃ©rification des donnÃ©es** :
   - Les donnÃ©es brutes et transformÃ©es seront disponibles dans le bucket S3 configurÃ©.
